# compose.gpu.yml
# GPU環境用の上書き設定ファイル
# 実行コマンド: PYTORCH_CUDA_VERSION=12.4 docker compose -f compose.yml -f compose.gpu.yml up -d --build
# 使用予定のサーバー名: gpu01, gpu02, wsl-ubuntu, via-tml2, tml-01-h100

services:
  app:
    build:
      # build.argsをGPU用の値で上書き
      args:
        # 1. 使用したいベースイメージを選択

        # gpu01
        # - BASE_IMAGE=nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04
        # PYTORCH_CUDA_VERSION=12.1 sudo docker compose -f compose.yml -f compose.gpu.yml build --progress=plain

        # wsl-ubuntu, via-tml2
        - BASE_IMAGE=nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
        # PYTORCH_CUDA_VERSION=12.4 sudo docker compose -f compose.yml -f compose.gpu.yml build --progress=plain

        # gpu02
        # - BASE_IMAGE=nvidia/cuda:12.8.0-cudnn-devel-ubuntu22.04
        # PYTORCH_CUDA_VERSION=12.8 sudo docker compose -f compose.yml -f compose.gpu.yml build --progress=plain

        # tml-01-h100
        # - BASE_IMAGE=nvidia/cuda:13.0.0-cudnn-devel-ubuntu22.04
        # PYTORCH_CUDA_VERSION=13.0 sudo docker compose -f compose.yml -f compose.gpu.yml build --progress=plain

        # 2. GPUを有効化
        - GPU_ENABLED=true

        # 3. コマンドラインからPYTORCH_CUDA_VERSIONを受け取る (デフォルトは12.4)
        - PYTORCH_CUDA_VERSION=${PYTORCH_CUDA_VERSION:-12.4}

    # .env の値をコンテナに渡す（通知用）
    env_file:
      - .env
    environment:
      - HOST_HOSTNAME=${HOSTNAME}
      - TZ=${TZ:-Asia/Tokyo}
      - NTFY_SERVER=${NTFY_SERVER:-https://ntfy.sh}
      - NTFY_TOPIC=${NTFY_TOPIC:-}
      - NTFY_TOKEN=${NTFY_TOKEN:-}

    # gpu01で共有メモリ不足になったら
    volumes:
      - /dev/shm:/dev/shm

    # GPUリソースを割り当てるdeployセクション
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
